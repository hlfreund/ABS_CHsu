---
title: "An Non-Assembly-Based Metagenomics Analysis Workflow using BioBakery Tools"
author: "Author: Linton Freund (hfreu002@ucr.edu)"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`" 
output:
  html_document:
    toc: true # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
    theme: flatly  # many options for theme, see here: https://www.datadreaming.org/post/r-markdown-theme-gallery/
    highlight: pygments  # specifies the syntax highlighting style
    code_folding: show
    toc_float: true
    collapsed: true
    smooth_scroll: true
fontsize: 15pt
always_allow_html: yes
bibliography: my_ref.bib
---

<style type="text/css">
  body{
  font-size: 13pt;
}
</style>

<!--
Render from R:
rmarkdown::render("Metagenomics_Workflow.Rmd", clean=TRUE, output_format="html_document")
R

Rendering from the command-line. To render to PDF format, use the argument setting: output_format="pdf_document".
$ Rscript -e "rmarkdown::render('Metagenomics_Workflow.Rmd', output_format='html_document', clean=TRUE)"

Add logo:
htmltools::img(src = knitr::image_uri("mylogo.png"), 
               alt = 'logo', 
               style = 'position:absolute; top:0; center:0; padding:10px;')
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

## Considerations before you begin
I was able to analyze these on a High Performance Computing cluster (HPCC) that uses a Slurm scheduler. The minimum amount of total memory I used (not per CPU, but overall) for each step in this workflow (i.e., each step as a separate 'job' via Slurm) was 100GB. Having enough memory is essential to running most of these programs, so please keep this in mind before embarking on this workflow!

These steps are also time consuming depending on your memory constraints, so do not be concerned if this takes a while. If you are running more than one step in a sitting, then I suggest loading **tmux** before you run your scripts. Here is a handy [tmux cheat sheet](https://tmuxcheatsheet.com/) that I refer to often. For more information on what tmux is and how to utilize it, check this [tmux crash course](https://thoughtbot.com/blog/a-tmux-crash-course) by Josh Clayton.

Additionally, you may need to change your path to each of these programs depending on where they are stored in your computer or HPCC. If you are running these steps locally (which, if you are, then you have one badass computer!), then you can skip the module loading lines in each step -- this is specifically for running these scripts on a HPCC.

## Submitting Scripts as Jobs with Slurm
If you are unsure as to how to set up the script for submitting on your HPCC, check the code chunk below. This is the information I use when submitting a job to our Slurm system. Again, this is specifically for a system that uses the Slurm scheduler. For more information on what the arguments mean and how to set up your job submissions, please refer to this handy [cheatsheet](https://slurm.schedmd.com/pdfs/summary.pdf) made by Slurm. 
```{bash slurm_job_prep, eval=FALSE}
#!/bin/bash -l

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4 # must match the # of threads if program allows threading (-t #)
##SBATCH --mem-per-cpu=100G # memory per cpu - * if threading, do not let this line run (use ##). Cannot ask for too much memory per cpu!
#SBATCH --mem=100GB # overall memory - if you're threading, keep this line
#SBATCH --time=1-00:00:00     # time requested; this example is 1 day, 0 hrs
#SBATCH --output=name_of_log_file.stdout # name of your log file
#SBATCH --mail-user=email_address@gmail.com # your email address 
#SBATCH --mail-type=ALL # will send you email updates when job starts and ends (and if it runs successfully or not)
#SBATCH --job-name="Name of Job 1/1/21" # name of your job for Slurm
#SBATCH --qos=hotel # quality of service which determines scheduling priority, job limits, etc
#SBATCH -p node_name_here # partition node name
```
When I don't know exactly what a program's output will look like, I will run the program via an interactive job on the HPCC. I also suggest running programs interactively if the program requires multiple lines of code to run, and you want to make sure each step has the correct input (whether it be a file, an object, or the output of a previous step in the code). For some more information on interactive jobs in a Slurm system, check out this [blog post](https://yunmingzhang.wordpress.com/2015/06/29/how-to-use-srun-to-get-an-interactive-node/) by Yunming Zhang.
This is how I set up an interactive job on the HPCC (that uses Slurm).
```{bash interactive_job, eval=FALSE}
#srun -p node_name_here --mem=500gb --time=1-00:00:00 --pty bash -l
srun --pty --nodes=1 --ntasks-per-node=1 --cpus-per-task=4 --mem=100gb -p hotel -q hotel -A htl119 -t 1-0:00:00 /bin/bash
# -p = partition
# -q = quality of service
# -A = account 
# -t = time requesting for job
# --pty = gives pseudo terminal that runs bash
# --nodes = # of nodes requesting
# --ntasks-per-node = # of tasks run per node for job
# --copus-per-task = # of threads per process
# --mem = overall memory being requested, not memory per CPU
# --time = overall time requested; 1 day, 0 hrs
# bash -l = setting bash as language
```
# Let's start the workflow!
## Check the quality of your sequences with `FastQC`
Before we jump into analyzing our sequences, it's always a good idea to check the quality of your sequences. `FastQC` [@Andrews] provides a comprehensive report on the quality of your sequences and is helpful for the following: identifying primers or adapters still attached to your sequences; determining the quality of your reverse reads; etc. This is also a helpful step for amplicon sequencing - in some cases, moving forward with only your forward reads may be best if the your reverse reads are of poor quality. 
```{bash fastqc_check, eval=FALSE}
# Load conda environment to run these programs if you need to
## for submitted job instead of interactive job, you would run source activate NameOfCondaEnvironment
conda activate NameOfCondaEnvironment

# Set the path to your sequences are located
workplace=/set/your/path/

# Create directory for specific sample results
if [[ ! -d ${workplace}/FastQC_PreTrim_Results ]]; then
    mkdir ${workplace}/FastQC_PreTrim_Results
fi

# Loop through fastq files
for FILE in ${workplace}/*.fastq.gz;
do
    # $FILE just represents the entire file path
    fastqc $FILE --outdir=${workplace}/FastQC_PreTrim_Results
    
done

# after you finish running FastQC, you can run multiqc on your FastQC results
multiqc ${workplace}/FastQC_PreTrim_Results -o ${workplace}/FastQC_PreTrim_Results/

# deactivate conda environment if you are done running your code
conda deactivate

```

FastQC will return a report assessing the "per sequence" and "per base" quality of your sequences. Below is an example of the "per base sequence quality" portion of the report. This report helps me to determine where I should trim my sequences as I move forward with the analysis 

![](fastqcReport.png)
<div align="center">Figure 1: FASTQ quality report for 1 Sample (R1 Only)</div></br>

## Adapter + Index Trimming with `bbduk`
Once we know the quality of our sequences, we can begin to process our sequences and prepare them for assembly. I used `bbduk` (from the BBMap Suite of tools created by the Joint Genome Institute, @Bushnell) to trim adapters and indexes from my sequences. Programs such as [`cutadapt`](https://cutadapt.readthedocs.io/en/stable/) and [`trimmomatic`](http://www.usadellab.org/cms/?page=trimmomatic) are useful as well. The benefit of `bbduk` is that this program compares **k-mers** (aka a substring of a sequence of length *k*) to literal sequences (i.e., adapter or primer sequences) and/or a reference of adapter sequences to identify and remove contaminant sequences from the reads.

`bbduk` has an option to add literal sequences to be identified and trimmed. Unknown nucleotides can be substituted with "N". This option is extremely helpful in situations when you may known your adapter sequences, but not your index sequences (that are sandwiched in between the adapter). For more on `bbduk` and the other programs offered by BBTools, please check out the [BBTools User Guide](https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/).

**NOTE**: Please remove or edit the literal sequences before running this code! 
```{bash seq_trimming, eval=FALSE}
module load BBMap/38.86 -- load module on SLURM system

workplace=/set/your/path
bbduksources=/path/to/BBdukResources

if [[ ! -d ${workplace}/Trimmed_Seqs ]]; then
    mkdir ${workplace}/Trimmed_Seqs
fi

# first, gunzip the FASTQ files
gunzip ${workplace}/*.fastq.gz

# then let's trim and decontaminate!
for i in ${workplace}/*_R1_001.fastq;
do
    f=$(basename $i) # use basename command to just get the file name rather than the entire file path for $i
    SAMPLE=${f%_R*} # remove everything that includes _R* using substring substitution %
    
    if [[ ! -f ${workplace}/Trimmed_Seqs/${SAMPLE}_R1_trim.fastq ]] && [[ ! -f ${workplace}/Trimmed_Seqs/${SAMPLE}_R2_trim.fastq ]]; then
    
        bbduk.sh -Xmx10g in1=${workplace}/${SAMPLE}_R1_001.fastq in2=${workplace}/${SAMPLE}_R2_001.fastq out1=${workplace}/Trimmed_Seqs/${SAMPLE}_R1_trim.fastq out2=${workplace}/Trimmed_Seqs/${SAMPLE}_R2_trim.fastq  ref=${bbduksources}/adapters.fa ftl=10 ftr=147 rcomp=t ktrim=r k=23 maq=10 minlength=120 trimpolyg=6 trimpolya=6 literal=NNNNNNNNNNNNNNNN,NNNNNNNNNNNNNNNN mink=11 hdist=1 tpe tbo
    fi
    
done

conda deactivate

# Lots of code/argument notes for usage below!
# More info here: https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/
# also http://seqanswers.com/forums/showthread.php?t=42776
# also also: https://github.com/BioInfoTools/BBMap/blob/master/sh/bbduk.sh
# ref ---> file provided by bbduk that holds collection of Illumina TruSeq adapters
# literal=(sequence here) ---> literal adapter sequences to remove; "N" represents any base -- in this case, they are indexes within the adapters
# rcomp=t ---> Rcomp looks for kmers and their reverse-complements, rather than just forward kmer, if set to true
# ktrim=r ---> “ktrim=r” is for right-trimming (3′ adapters);
## In ktrim=r mode, once a reference kmer is matched in a read, that kmer and all the bases to the right will be trimmed, leaving only the bases to the left; this is the normal mode for adapter trimming.
# k=23 ---> look for kmer that is 23 bp long
# trimpolyg=6 --> trim poly G sequences by 6 bps; can also trim polyA or polyT tails
# mink=11 ---> in addition to kmers of x length, look for shorter kmers with lengths 23 to 11 (in this case)
# maq=10 ---> This will discard reads with average quality below 10
# hdist=1 ---> hamming distance of 1 (for identifying kmers)
# mlf=50 ---> (minlengthfraction=50) would discard reads under 50% of their original length after trimming
# trimq=10 ---> quality-trim to Q10 using the Phred algorithm, which is more accurate than naive trimming.
# qtrim=r ---> means it will quality trim the right side only [happens after all base kmer operations]
# tpe ---> which specifies to trim both reads to the same length
# tbo ---> which specifies to also trim adapters based on pair overlap detection using BBMerge (which does not require known adapter sequences)
# mm ----> Maskmiddle ignores the middle base of a kmer, can turn off with mm=f
# ftl ----> (force) trim the left most 5 bases

# more on hdist...
# A 4.6Mbp genome like E.coli contains around 4.6 million unique kmers. If a hamming distance is used, such as hdist=1, then the number of kmers stored will be
# multiplied by 1+(3*k)^hdist. So, for E.coli with K=31 and hdist=0, there are 4554207 kmers stored, using around 140MB, taking about 0.5 seconds; with
# hdist=1, there are 427998710 kmers stored (94 times as many), using 15GB, and taking 104 seconds
# BBDuk needs around 20 bytes per kmer

# BBDuk supports kmers of length 1-31. The longer a kmer, the high the specificity
# Note that it is impossible to do kmer matching for reference sequences that are shorter than K.
# When kmer filtering, you can specify a kmer length greater than 31 (such as k=40) for even higher specificity.
# For k=40, BBDuk will consider a read to match the reference if there are 10 consecutive 31-mer matches. This is not exactly the same as actually matching 40-mers, but is very similar.
# Example Usage
## bbduk.sh -Xmx10g in1=${workplace}/16S_Seqs/${SAMPLE}_R1.fastq in2=${workplace}/16S_Seqs/${SAMPLE}_R2.fastq out1=${workplace}/Trimmed_Seqs/16S_Trimmed/${SAMPLE}_R1_clean.fastq out2=${workplace}/Trimmed_Seqs/16S_Trimmed/${SAMPLE}_R2_clean.fastq literal=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG,GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG ref=/${bbduksources}/adapters.fa ftl=5 ftr=265 rcomp=t ktrim=r k=23 maq=10 minlength=250 mink=11 hdist=1 tpe tbo

```

## Map Reads back to Assembly with `BWA-MEM`
To improve the assembly of our MAGs, we need to map our reads back to our contig assemblies. There are several programs that can be used to do this including `Bowtie2` and `bbmap`, but I decided to use `BWA-MEM` [@Li2013]. The `BWA-MEM` aligner performs **local sequence alignments** (based on the [Smith-Waterman algorithm](https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm)). Both `BWA-MEM` and `Bowtie2` are aligners that utilize the **Burrows-Wheeler transform** algorithm to efficiently store sequence data. If you're interested in how the Burrows-Wheeler transformation works, and its other implementations, check out this [GeeksforGeeks article](https://www.geeksforgeeks.org/burrows-wheeler-data-transform-algorithm/) on the algorithm.

We can then convert our seauence alignment mapping files (aka SAM files) to binary versions of these files (aka BAM files) using `samtools` [@Li2009]. BAM files are helpful for compressing all of this sequence alignment information.

**NOTE: You need indexed (ordered) BAM files (aka .bam.bai files) in order to bin your sequences!**
```{bash map_reads_to_assembly, eval=FALSE}
Path="/set/your/path/" # initiate your path for later
today=$(date "+%m.%d.%y") # date is the command to get today's date, and the "+%m_%d_%y" will print it in month_day_year format
module load bwa-mem2/2.0
module load samtools/1.11
module load bwa/0.7.17

for i in *_contigs.fasta;
do
    SAMPLE=$(echo ${i} | sed "s/_contigs.fasta//")
    echo ${SAMPLE} " -- mapping reads to assembled contigs with BWA-MEM"
    
    ## First, build the index aka reference for mapping reads to
    bwa index ${SAMPLE}_contigs.fasta

    ### Map reads back using local alignment
    if [[ ! -f ${Path}/${SAMPLE}_contigs_aln_pe.sam ]]; then
    
        bwa mem ${SAMPLE}_contigs.fasta ${SAMPLE}_L001_R1_norm_EC.fastq.gz ${SAMPLE}_L001_R2_norm_EC.fastq.gz -t 8 > ${SAMPLE}_contigs_aln_pe.sam
        #bwa-mem2 mem -t 8 ${SAMPLE}_c_assembly ${SAMPLE}_L001_R1_norm_EC.fastq.gz ${SAMPLE}_L001_R2_norm_EC.fastq.gz > ${SAMPLE}_contigs_aln-pe.sam
    fi
       
done

for i in *_contigs_aln_pe.sam;
do
    SAMPLE=$(echo ${i} | sed "s/_contigs_aln_pe.sam//")
    echo ${SAMPLE} " -- switching SAM file to BAM file"
  
    ### Map reads back using local alignment
    if [[ -f ${Path}/${SAMPLE}_contigs_aln_pe.sam ]]; then
        
        ## Convert SAM file to BAM file with samtools
        samtools view -S -b ${SAMPLE}_contigs_aln_pe.sam > ${SAMPLE}_unsort_bwa.bam  ## views & converts SAM to BAM file
        samtools sort ${SAMPLE}_unsort_bwa.bam -o ${SAMPLE}_sorted.bam ## sorts BAM file; sort alignments by leftmost coordinates, or by read name when -n is used
        samtools index  ${SAMPLE}_sorted.bam ## indexes BAM file
        samtools flagstat -@ 8 -O tsv ${SAMPLE}_sorted.bam > ${SAMPLE}_sorted_stats.tsv
        samtools coverage  ${SAMPLE}_sorted.bam -o ${SAMPLE}_sorted_coverage.tsv
        samtools depth  ${SAMPLE}_sorted.bam -o ${SAMPLE}_sorted_depth.tsv
    fi
       
done
```

## Bin Contigs with `MetaBAT`
To identify genomes in our metagenomes, we need to cluster sequences together into bins using the contigs assembled by `metaSPades` and the BAM alignment file created by `BWA-MEM`. `MetaBAT` [@Kang2019] does this by calculating the tetranucleotide frequency distance probability (TDP) and abundance of (aka mean base coverage) for pairs of contigs in each assembly [@Kang2015]. 
```{bash bin_contigs, eval=FALSE}
Path="/set/your/path/" # initiate your path for later
today=$(date "+%m.%d.%y") # date is the command to get today's date, and the "+%m_%d_%y" will print it in month_day_year format
module load metabat/0.32.4 # load the module on your computing cluster system (ignore this if running locally)

for i in *_contigs.fasta;
do
    SAMPLE=$(echo ${i} | sed "s/_contigs.fasta//")
    echo ${SAMPLE} " -- binning contigs in this metagenome"
    
    runMetaBat.sh -t 4 -m 1500 ${SAMPLE}_contigs.fasta ${SAMPLE}_bam.bam
    
done

## If you're running metaBAT locally (not on Slurm), do this: 
for i in *_contigs.fasta;
do
    SAMPLE=$(echo ${i} | sed "s/_contigs.fasta//")
    echo ${SAMPLE} " -- binning contigs in this metagenome"
    
    jgi_summarize_bam_contig_depths --outputDepth ${SAMPLE}_depth.txt ${SAMPLE}_bam.bam
    metabat2 -t 4 ${SAMPLE}_contigs.fasta -a ${SAMPLE}_depth.txt -o ${SAMPLE}_contig_bins/bin -v

done

## Move all bin files into a directory of bins
for i in *_contigs.fasta;
do
    SAMPLE=$(echo ${i} | sed "s/_contigs.fasta//")
    echo ${SAMPLE}
    find . -name "${SAMPLE}*fa" -exec mv -t ./${SAMPLE}_bins_metabat {} +

done
```

## Check Bin Completeness with `CheckM`
To determine which bins to annotate downstream, we have to assess the quality and accuracy of our bins. `CheckM` [@Parks2015] reports all kinds of statistics regarding our assembly and gives us a preliminary taxonomic assignment for each genome bin. 
```{bash check_bins_checkm, eval=FALSE}
Path="/set/your/path/" # initiate your path for later
today=$(date "+%m.%d.%y") # date is the command to get today's date, and the "+%m_%d_%y" will print it in month_day_year format
module load checkm/1.1.3 # load the module on your computing cluster system (ignore this if running locally)

checkm taxon_set domain Bacteria Bacteria_marker_file
checkm taxon_set domain Archaea Archaea_marker_file

for i in *_contigs.fasta;
do
    SAMPLE=$(echo ${i} | sed "s/_contigs.fasta//")
    echo ${SAMPLE}
    
    if [[ ! -f ${Path}/${SAMPLE}_checkm_lineage_${today}/${SAMPLE}_checkm_lineage_results_${today}.txt ]]; then
        checkm lineage_wf -t 8 -x fa ./${SAMPLE}_bins_metabat ./${SAMPLE}_checkm_lineage_${today} --file ${SAMPLE}_checkm_lineage_results_${today}.txt --tab_table
        ## checkm lineage_wf -t {#threads} -x {file format, aka fa} {BIN_DIRECTORY} {OUTPUT_DIRECTORY} -f {output file} --tab_tablels -
    fi
    
    if [[ ! -f ${Path}/${SAMPLE}_checkm_qa_${today}.tsv ]]; then
        checkm qa -o 2 -f ./${SAMPLE}_checkm_qa_${today}.tsv --tab_table ./${SAMPLE}_checkm_lineage_${today}/lineage.ms ./${SAMPLE}_checkm_lineage_${today}/
        # checkm qa --output_format 2 -f {output_file} --table_table {/path/to/marker/file(either lineage.ms if used checkm lineage_wf, or a taxon specific ms used for analyze command} {/path/to/checkm_lineage (or taxonomy) output}
    fi
    
    if [[ ! -f ${Path}/${SAMPLE}_checkm_coverage_${today}.tsv ]]; then
        checkm coverage -x fa -q ./${SAMPLE}_bins_metabat ${SAMPLE}_checkm_coverage_${today}.tsv ${SAMPLE}_sorted.bam
    fi
    
    if [[ ! -d ${Path}/./${SAMPLE}_bacteria_marker_bins ]] && [[ ! -d ${Path}/./${SAMPLE}_archaea_marker_bins ]]; then
        checkm analyze Bacteria_marker_file ./${SAMPLE}_bins_metabat -x fa -t 4 ./${SAMPLE}_bacteria_marker_bins
        checkm qa Bacteria_marker_file ./${SAMPLE}_bacteria_marker_bins -o 2 -f ${SAMPLE}_bacteria_marker_results.txt --tab_table
    
        checkm analyze Archaea_marker_file ./${SAMPLE}_bins_metabat -x fa -t 4 ./${SAMPLE}_archaea_marker_bins
        checkm qa Archaea_marker_file ./${SAMPLE}_archaea_marker_bins -o 2 -f ${SAMPLE}_archaea_marker_results.txt --tab_table
    fi
done
```

## Compare Bin Assemblies with `MetaQUAST`
MetaQUAST [@Mikheenko2016] helps to compare bin assemblies per sample to provide some summary statistics per bin, per sample. The MetaQUAST report includes helpful information such as the number of misassemblies, GC%, N50 (the length of all of the contigs that cover at least half of the assembly), the largest alignment, etc. 
```{bash compare_bins, eval=FALSE}
today=$(date "+%m.%d.%y") # date is the command to get today's date, and the "+%m_%d_%y" will print it in month_day_year format
module load QUAST/5.0.0-dev # ignore this and the sbatch commands if running this locally or not on a Slurm system

for i in *_contigs.fasta;
do
    SAMPLE=$(echo ${i} | sed "s/_contigs.fasta//")
    echo "Checking ${SAMPLE} assembled CONTIGS with metaQUAST ${today}"
    
    metaquast.py -t 4 -f -o ./${SAMPLE}_metaQUAST_contigs_${today} --gene-finding ${SAMPLE}_contigs.fasta
    # if using sam file...
    # python metaquast.py -t 4 --glimmer -o ./${SAMPLE}_metaQUAST_contig_${today}--sam ${SAMPLE}_sam.sam ${SAMPLE}_contigs.fasta
    # if using bam file...
    # python metaquast.py -t 4 -f -o ./${SAMPLE}_metaQUAST_wbam_${today} --gene-finding --bam ${SAMPLE}_sorted.bam ${SAMPLE}_contigs.fasta
    
    echo "Checking ${SAMPLE} assembled SCAFFOLDS with metaQUAST ${today}"
    metaquast.py -t 4 -f -o ./${SAMPLE}_metaQUAST_scaffolds_${today} --gene-finding -s ${SAMPLE}_scaffolds.fasta

done
```

## Select the Best Bins for Gene Predictions
Determining which bins to use for taxonomic and functional annotation is a crucial, anxiety-inducing step! Typically, high-quality draft MAGs have > 90% completeness and < 5 % contamination. However, I have seen some folks using bins that were deemed 50% complete. I recommend reading @Chen2020 and @Bowers2017 for more information regarding recommended MAG completeness and contamination thresholds. 
```{bash select_good_bins, eval=FALSE}
today=$(date "+%m.%d.%y") # date is the command to get today's date, and the "+%m_%d_%y" will print it in month_day_year format

# double check the dates given on the names of the files before you start!
for i in $(ls *_checkm_qa_06.24.21.tsv); # change to your output file names from when you ran CheckM
do
    SAMPLE=$(echo $i | sed "s/_checkm_qa_06.24.21.tsv//") # double
    echo "${SAMPLE}: separating bins based on quality -- CheckM results"
    #sed -i 's/_contigs.fasta.metabat-bins-_-t_4_-m_1500./_bin_/g' ${i}
    
    awk -F '\t' 'BEGIN {OFS="\t"} { if ( NR == 1 ) print "Sample_ID", "Bin_Num" ,"Taxa_Level", "Marker_Lineage", "Lineage_ID", "Genome_Num", "Completeness", "Contamination", "Strain_Heterogeneity", "GC_Content"; else if ($6 >=80 && $7 < 5) print $1,$2,$3,$6,$7,$8,$19}' ${i} | sed 's/ /\t/g' | sed 's/_bin/\tBin/g' | sed 's/__/\t/g' > ${SAMPLE}_bins.tsv
    ## Bins over >70% complete, <5% contamination for annotation
    
done

## Moving bins of decent quality described in ${SAMPLE}_bins.tsv to separate folder for downstream annotations
for SAMPDIR in $(ls -d *_bins_metabat)
do
    SAMPLE=$(basename $SAMPDIR _bins_metabat)
    if [[ ! -d ${SAMPLE}_good_bins ]]; then
        mkdir ${SAMPLE}_good_bins
    fi
    
    #echo $ffa
    echo "Copying quality bins (>70% completeness, <5% contamination) into ${SAMPLE}_good_bins"
    for f in $(awk 'FNR == 1 {next} NR > 1{ print $1 }' ${SAMPLE}_bins.tsv)
    do
         ffa=$(echo $f.fa)
         #mkdir ${SAMPLE}_quality_bins
         cp "${SAMPDIR}/${ffa}" "./${SAMPLE}_good_bins"

         # if you want to keep track of the date in which you ran this step, just add ${today} to your ${SAMPLE}_good_bins directory name
    done
    
done

# Notes to self about AWK
# -F ' ' == field separator of input doc is a space
# NR == 1 == if first line is 1, do _____
```

## MAG Annotation 

### Taxonomic Annotation with `GTDB-Tk`
Now that we have selected some good quality bins, we can perform taxonomic annotation with the Genome Taxonomy Database Toolkit, aka `GTDB-Tk` [@Chaumeil2020]. 
In case you're curious, these are the [criteria](https://gtdb.ecogenomic.org/faq) that the Genome Taxonomy Database (GTDB) follow when choosing genomes to add to their database:

1. CheckM completeness estimate >50%
1. CheckM contamination estimate <10%
1. quality score, defined as completeness - 5*contamination, >50
1. contain >40% of the bac120 or arc122 marker genes
1. contain <1000 contigs
1. have an N50 >5kb
1. contain <100,000 ambiguous bases
```{bash gtdbtk_annotation, eval=FALSE}
today=$(date "+%m.%d.%y") # date is the command to get today's date, and the "+%m_%d_%y" will print it in month_day_year format
module load gtdbtk/1.5.0 # load the module on your computing cluster system (ignore this if running locally)

for SAMPDIR in $(ls -d *_good_bins)
do
  SAMPLE=$(basename $SAMPDIR _good_bins)
  gtdbtk classify_wf --genome_dir $SAMPDIR -x fa --out_dir gtdbtk_${SAMPLE}_${today}
  
done
```

### Functional Annotation with `Prokka`
Identifying putative functional traits in our MAGs can be done with `Prokka` [@Seemann2014]. 
```{bash prokka_annotation, eval=FALSE}
module load rnammer/1.2 # for rRNA searches
module load barrnap/0.9 # for rRNA searches
module load prokka/1.14.5
#today=$(date "+%m.%d.%y") # date is the command to get today's date, and the "+%m_%d_%y" will print it in month_day_year format

for FILE in *_good_bins/*.fa;
do
    fa=$(basename $FILE)
    y=${fa%.fa*}
    BIN=${y#EA_Pool_*}
    echo $BIN
    
    # Run Prokka
    prokka --outdir ${BIN}_prokka --prefix ${BIN} --metagenome $FILE
    
    # Remove sequences from GFF file and rename these edited files
    sed '/^##FASTA/Q' ${BIN}_prokka/${BIN}.gff > ${BIN}_prokka/${BIN}_clean.gff # _clean.gff --> no sequences in gff file

done

# Create folder to store all Prokka Results
if [[ ! -d ./Prokka_Results ]]; then
    mkdir Prokka_Results
fi

# Move all Prokka results to this folder
mv *_prokka ./Prokka_Results

# Create directory to store Prokka GFFs
if [[ ! -d ./Prokka_Results/Prokka_GFFs ]]; then
    mkdir ./Prokka_Results/Prokka_GFFs
fi

# Copy GFFs (clean GFFs) to Prokka_GFFs folder
for i in ./Prokka_Results/*_prokka/*_clean.gff;
do
    cp $i ./Prokka_Results/Prokka_GFFs
done
```

### Retrieve EC and COG #s from Prokka Output`
```{bash get_Prokka_results, eval=FALSE}
# Create directories to store EC numbers and COG IDs (for MinPath analysis)
if [[ ! -d ./Prokka_Results/Prokka_EC ]] && [[ ! -d ./Prokka_Results/Prokka_COGs ]]; then
    mkdir ./Prokka_Results/Prokka_EC
    mkdir ./Prokka_Results/Prokka_COGs
fi

# Create EC and COG files for MinPath analysis from Prokka GFF files
for FILE in ./Prokka_Results/*_prokka/*_clean.gff;
do
    f=$(basename $FILE)
    y=${f%_clean*}
    BIN=${y#EA_Pool_*}
    
    # Create EC File
    grep "eC_number=" ${FILE} | cut -f9 | cut -f1,2 -d ';'| sed 's/ID=//g'| sed 's/;eC_number=/\t/g' > ./Prokka_Results/${BIN}_prokka/${BIN}_prokka.ec
    
    # Create COG file
    egrep "COG[0-9]{4}" ${FILE} | cut -f9 | sed 's/.\+COG\([0-9]\+\);locus_tag=\(PROKKA_[0-9]\+\);.\+/\2\tCOG\1/g' > ./Prokka_Results/${BIN}_prokka/${BIN}_prokka.cog

done

# Copy EC and COG files to separate directories
for i in ./Prokka_Results/*_prokka/*.ec;
do
    cp $i ./Prokka_Results/Prokka_EC
done

for i in ./Prokka_Results/*_prokka/*.cog;
do
    cp $i ./Prokka_Results/Prokka_COGs
done
```

# About Me
My name is Hannah and my pronouns are they/them. I am currently a PhD Student at UC Riverside in the [Genetics, Genomics, and Bioinformatics](https://ggb.ucr.edu/) PhD program and a member of [Dr. Emma Aronson's lab](https://profiles.ucr.edu/app/home/profile/emmaa). 

If you have any questions regarding this workflow and the scripts I used, do not hesitate to contact [me](mailto:hfreu002@ucr.edu?subject=Metagenome Assembly Workflow). Or, if you'd like to talk bioinformatics and all things 'omics, I would love that too!

If you'd like to learn more about my research background and check out some of my other work, feel free to visit my [website](https://hlfreund.github.io/). It is a work in progress, just fyi!

For some personal information about me, check out my page with [500 Queer Scientists](https://500queerscientists.com/hannah-freund/), and my [twitter](https://twitter.com/UrFreundHannah) if you're curious.

<div align="center">**Thank you so much for checking out my workflow!**</div>
<center>
[![website](mylogo.png)](https://hlfreund.github.io/)
</center>

# Version Information
```{r sessionInfo}
sessionInfo()
```

# References
